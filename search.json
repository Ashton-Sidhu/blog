[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ashton’s Tech Tales",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\nUsing Databricks & Prefect to automate your ETL tasks\n\n\n\n\n\n\nNov 2, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nCreate a Python wrapper for functions written in Go.\n\n\n\n\n\n\nOct 25, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nGuide and tips on how to get started/make a contribution during Hacktoberfest.\n\n\n\n\n\n\nOct 8, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nRun your Jupyter Notebook as a stand alone web app.\n\n\n\n\n\n\nOct 3, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nSetup your own Big Data infrastructure at home.\n\n\n\n\n\n\nApr 17, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nBuild interactive graph network visualizations.\n\n\n\n\n\n\nMar 27, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIntroduction to Aethos Modelling.\n\n\n\n\n\n\nMar 14, 2020\n\n\nAshton Sidhu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/ctypes/2020-10-25-ctypes-go.html",
    "href": "posts/ctypes/2020-10-25-ctypes-go.html",
    "title": "Tutorial: Integrating Python with Compiler Languages",
    "section": "",
    "text": "Python - a programming language that is human readable, easy to learn, and most importantly, easy to use. It is no wonder that it is one of the most popular languages today. From being able to build web applications, create automation scripts, analyze data and build machine learning models, Python is a jack of all trades. As with anything it has its short comings which include its speed and lack of granular access to machine hardware. This is due to Python being an interpreter-based language and not a compiler-based language.\nLuckily for us, there is a way to solve that problem. Python has a native library that allows you to use functions built in compiled languages. This gives you the ability to leverage both the powers of Python and compiler based languages. Some of the most popular data science libraries such as Pandas and XGBoost have modules written in C/C++ (compiler-based languages) to overcome Python’s speed and performance issues while still having a user friendly Python API.\nIn this post, we are going to walk through interpreter based languages vs. compiler based languages, Python’s built in ctypes module and an example of how to use modules built from a compiled language. From there you will be able to integrate Python with other languages in your data science or machine learning projects."
  },
  {
    "objectID": "posts/ctypes/2020-10-25-ctypes-go.html#interpreters-vs.-compilers",
    "href": "posts/ctypes/2020-10-25-ctypes-go.html#interpreters-vs.-compilers",
    "title": "Tutorial: Integrating Python with Compiler Languages",
    "section": "Interpreters vs. Compilers",
    "text": "Interpreters vs. Compilers\nAs I mentioned above, Python is an interpreted language. In order to execute code on a machine, the code first has to get translated to “machine code”.\n\n\n\n\n\n\nNote\n\n\n\nMachine code is just binary or hexadecimal instructions.\n\n\nThe main difference between interpreted and compiled languages is that interpreted languages get executed line by line and passed through a translator that converts each line to machine code.\nCompilers require a build step that translates all the code at once into an application (or binary) that can be executed. During the build phase there is a compiler that will optimize the translation from written code to machine code. The compiler’s optimizations are one of compiled based-languages are faster than interpreter-based languages. Common compiled languages are C, C++ and Go.\n\n\n\n\n\n\nNote\n\n\n\nThis a high level comparison between interpreters and compilers. More in-depth knowledge would require individual research or a separate topic entirely."
  },
  {
    "objectID": "posts/ctypes/2020-10-25-ctypes-go.html#c-types",
    "href": "posts/ctypes/2020-10-25-ctypes-go.html#c-types",
    "title": "Tutorial: Integrating Python with Compiler Languages",
    "section": "C Types",
    "text": "C Types\nPython has built in libraries to be able to call modules built in compiled languages. It gives developers the ultimate flexibility to use compiled languages for tasks that Python isn’t well equipped to handle - all the while still building the core of the application in Python.\nThe main library that we will be using to interact with modules from a compiled application is ctypes. It provides C compatible data types and calling functions from applications built in compiled languages. It gives you the ability to wrap these languages in pure Python. To be able to do this, Python first has to convert its function argument types into C native types. This allows it to be compatible with compiled language function’s argument types. The figure below explains the process at a high level when interacting with functions from compiled languages.\n\n\n\n\n\n\nNote\n\n\n\nThe example in the next section will be using Go so the image is Go specific, but the principle applies to the other compiled languages."
  },
  {
    "objectID": "posts/ctypes/2020-10-25-ctypes-go.html#walk-through",
    "href": "posts/ctypes/2020-10-25-ctypes-go.html#walk-through",
    "title": "Tutorial: Integrating Python with Compiler Languages",
    "section": "Walk Through",
    "text": "Walk Through\nBelow is a Go function I wrote that gets the closing price of a stock using the Alpha Vantage API. I’ll go over the key lines that allow us to create a Python wrapper for it.\npackage main\n\nimport \"C\"\n\nimport (\n    \"encoding/json\"\n    \"fmt\"\n    \"io/ioutil\"\n    \"log\"\n    \"net/http\"\n    \"os\"\n    \"time\"\n)\n\n// APIKEY ... Alpha Vantage API key, stored as env variable\nvar APIKEY string = os.Getenv(\"ALPHA_API_KEY\")\n\n// ENDPOINT ... Alpha Vantage API daily stock data endpoint\nvar ENDPOINT string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED\"\n\n// Data ... Data structure\ntype Data struct {\n    MetaData   MetaData               `json:\"Meta Data\"`\n    TimeSeries map[string]interface{} `json:\"Time Series (Daily)\"`\n}\n\n// MetaData ... Stock metadata structure\ntype MetaData struct {\n    Info        string `json:\"1. Information\"`\n    Symbol      string `json:\"2. Symbol\"`\n    LastRefresh string `json:\"3. Last Refreshed\"`\n    OutputSize  string `json:\"4. Output Size\"`\n    TimeZone    string `json:\"5. Time Zone\"`\n}\n\n// FinData ... Daily Financial Data json structure\ntype FinData struct {\n    Open       string `json:\"1. open\"`\n    High       string `json:\"2. high\"`\n    Low        string `json:\"3. low\"`\n    Close      string `json:\"4. close\"`\n    AdjClose   string `json:\"5. adjusted close\"`\n    Volume     string `json:\"6. volume\"`\n    DivAmount  string `json:\"7. dividend amount\"`\n    SplitCoeff string `json:\"8. split coefficient\"`\n}\n\nfunc main() {}\n\n//export getPrice\nfunc getPrice(ticker *C.char, date *C.char) *C.char {\n\n    tickerDate := C.GoString(date)\n    stock := C.GoString(ticker)\n\n    query := fmt.Sprintf(\"%s&symbol=%s&apikey=%s\", ENDPOINT, stock, APIKEY)\n\n    client := http.Client{\n        Timeout: time.Second * 10, // Timeout after 5 seconds\n    }\n\n    req, err := http.NewRequest(http.MethodGet, query, nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    req.Header.Set(\"User-Agent\", \"stock-api-project\")\n\n    resp, getErr := client.Do(req)\n    if getErr != nil {\n        log.Fatal(getErr)\n    }\n\n    defer resp.Body.Close()\n\n    respBody, _ := ioutil.ReadAll(resp.Body)\n\n    dailyData := Data{}\n    json.Unmarshal(respBody, &dailyData)\n\n    // Encode Interface as bytes\n    dailyFinDataMap := dailyData.TimeSeries[tickerDate]\n    dfdByte, _ := json.Marshal(dailyFinDataMap)\n\n    // Map interface to FinData struct\n    dailyFinData := FinData{}\n    json.Unmarshal(dfdByte, &dailyFinData)\n\n    return C.CString(dailyFinData.AdjClose)\n}\nimport “C” - Import the C package (aka cgo) to have access to C data types.\nfunc main() {} - An empty main function to ensure we still have an executable.\n//export getPrice - Export the function so that we can expose it to be accessed by Python.\nfunc getPrice(ticker *C.char, date *C.char) *C.char { - The function arguments need to be C types.\ntickerDate := C.GoString(date) - Convert the function arguments to their native Go types.\nreturn C.CString(dailyFinData.AdjClose) - The return value has to be a native C type.\nTo be able to use this in Python we have to compile this program, go build -o stock-api.so -buildmode=c-shared main.go\nBelow is the associating Python wrapper that calls our exported Go function getPrice.\nfrom ctypes import *\n\ndef get_price(ticker: str) -> Dict[str, str]:\n    \"\"\"Gets the adjusted closing price of all stocks.\"\"\"\n\n    lib = cdll.LoadLibrary(\"./stock-api.so\")\n    lib.getPrice.argtypes = [c_char_p, c_char_p]\n    lib.getPrice.restype = c_char_p\n\n    curr_date = str(date.today())\n    price = lib.getPrice(ticker.encode(), curr_date.encode()).decode()\n\n    return price\nfrom ctypes import * - Import everything from ctypes as per the documentation.\nlib = cdll.LoadLibrary(\"./stock-api.so\") - Load in the binary or compiled application.\nlib.getPrice.argtypes = [c_char_p, c_char_p] - Set the getPrice function argument types to the corresponding C types.\nlib.getPrice.restype = c_char_p - Set the return type to the corresponding C type.\nprice = lib.getPrice(ticker.encode(), curr_date.encode()).decode() - We call the getPrice function and convert the Python string arguments into bytes that can be passed to the Go function by calling .encode. We then receive the output from Go function as bytes so we decode it to convert it to a Python string."
  },
  {
    "objectID": "posts/ctypes/2020-10-25-ctypes-go.html#conclusion",
    "href": "posts/ctypes/2020-10-25-ctypes-go.html#conclusion",
    "title": "Tutorial: Integrating Python with Compiler Languages",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have most of the knowledge you need to get started creating wrappers for compiled language functions. No longer are you bound by the cons of interpreter based languages. You can create modules in languages that better suit your use case, and then fall back on Python to build out a user friendly API or the core of your application. Happy coding!"
  },
  {
    "objectID": "posts/ctypes/2020-10-25-ctypes-go.html#feedback",
    "href": "posts/ctypes/2020-10-25-ctypes-go.html#feedback",
    "title": "Tutorial: Integrating Python with Compiler Languages",
    "section": "Feedback",
    "text": "Feedback\nI encourage any and all feedback about any of my posts and tutorials. You can message me on twitter or e-mail me at sidhuashton@gmail.com ."
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "",
    "text": "As a data scientist, rapid experimentation is extremely important. If an idea doesn’t work it’s best to fail quickly and find out sooner rather than later.\nWhen it comes to modelling, rapid experimentation is already fairly simple. All model implementations follow the same API interface so all you have to do is initialize the model and train it. The problem comes now when you have to interpret, track, and compare each model.\nDo you make a large notebook with all your models and scroll up and down or use a table of contents to see the results of different models? Do you create a different notebook for each model and then flip back and forth between notebooks? How do you track iterations of the models if you start tweaking the parameters? Where do you store artifacts to revisit at a later date or for further analysis?\nI’m going to demonstrate a way to address these problems by training multiple models, each with 1 line of code, view the overall results easily, analyze the models, interpret the models, track the models in MLFlow and serve them using Aethos."
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#prepping-the-data",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#prepping-the-data",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Prepping the Data",
    "text": "Prepping the Data\nWe’ll use Aethos to quickly prep the data. For more information on how to analyze and transform your datasets with Aethos, check out my previous blog post here. Load the Titanic training data from the Aethos repo.\nimport aethos as at  \nimport pandas as pd\n\ndata = pd.read_csv('https://raw.githubusercontent.com/Ashton-Sidhu/aethos/develop/examples/data/train.csv')\nPass the data into Aethos.\ndf = at.Data(data, target_field='Survived')\n\nThe focus of this post is modelling so let’s quickly preprocess the data. We’re going to use the Survived, Pclass, Sex, Age, Fare and Embarked features. insert here\ndf.drop(keep=['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked'])\ndf.standardize_column_names()\nReplace the missing values in the Age and the Embarked columns.\ndf.replace_missing_median('age')  \ndf.replace_missing_mostcommon('embarked')\nNormalize the values in the Age and Fare columns and One Hot Encode the Sex, Pclass and Embarked features.\ndf.onehot_encode('sex', 'pclass', 'embarked', keep_col=False)  \ndf.normalize_numeric('fare', 'age')\n\n\nWith Aethos, the transformer is fit to the training set and applied to the test set. With just one line of code both your training and test set have been transformed."
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#modelling",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#modelling",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Modelling",
    "text": "Modelling\nTo train Sklearn, XGBoost, LightGBM, etc. models with Aethos, first transition from the data wrangling object to the Model object.\nmodel = at.Model(df)\nThe model object behaves the same way as the Data object so if you have data that has already been processed, you can initiate the Model object the same way as the Data object.\nNext, we’re going to enable experiment tracking with MLFlow.\nat.options.track_experiments = True\nNow that everything is set up, training a model and getting predictions is as simple as this:\nlr = model.LogisticRegression(C=0.1)\n\nTo train a model with the optimal parameters using Gridsearch, specify the gridsearch parameter when initializing the model.\nlr = model.LogisticRegression(gridsearch={'C': [0.01, 0.1]}, tol=0.001)\n\nThis will return the model with the optimal parameters defined by your Gridsearch scoring method.\nFinally, if you want to cross validate your models there are a few options:\nlr = model.LogisticRegression(cv=5, C=0.001)\nThis will perform 5-fold cross validation on your model and display the mean score as well as the learning curve to help gauge data quality, over-fitting, and under-fitting.\n\nYou can also use it with Gridsearch:\nlr = model.LogisticRegression(cv=5, gridsearch={'C': [0.01, 0.1]}, tol=0.001)\nThis will first use Gridsearch to train a model with the optimal parameters and then cross validate it. Currently supported cross validation methods are k-fold and stratified k-fold.\n\nTo train multiple models at once (in series or in parallel) specify the run parameter when defining your model.\nlr = model.LogisticRegression(cv=5, gridsearch={'C': [0.01, 0.1]}, tol=0.001, run=False)\nLet’s queue up a few more models to train:\nmodel.DecisionTreeClassification(run=False)  \nmodel.RandomForestClassification(run=False)  \nmodel.LightGBMClassification(run=False)\nYou can view queued and trained models by running the following:\nmodel.list_models()\n\nTo now run all queued models, by default in parallel:\ndt, rf, lgbm = model.run_models()\nYou can now go grab a coffee or a meal while all your models are trained simultaneously!"
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#analyzing-models",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#analyzing-models",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Analyzing Models",
    "text": "Analyzing Models\nEvery model you train by default has a name. This allows you to train multiple versions of the same model with the same model object and API, while still having access to each individual model’s results. You can see each model’s default name in the function header by pressing Shift + Tab in Jupyter Notebook.\n\nYou can also change the model name by specifying a name of your choosing when initializing the function.\nFirst, let’s compare all the models we’ve trained against each other:\nmodel.compare_models()\n\nYou can see every models’ performance against a wide variety of metrics. In a data science project there are predefined metrics you want to compare models against (if you don’t, you should). You can specify those project metrics through the options.\nat.options.project_metrics = ['Accuracy', 'Balanced Accuracy', 'Zero One Loss']\nNow when you compare models, you’ll only see the project metrics.\nmodel.compare_models()\n\nYou can also view the metrics just for a single model by running the following:\ndt.metrics() # Shows metrics for the Decision Tree model  \nrf.metrics() # Shows metrics for the Random Forest model  \nlgbm.metrics() # Shows metrics for the LightGBM model\nYou can use the same API to see each models’ RoC Curve, Confusion matrix, the index of misclassified predictions, etc.\ndt.confusion_matrix(output_file='confusion_matrix.png') # Confusion matrix for the Decision Tree model\n\nrf.confusion_matrix(output_file='confusion_matrix.png') # Confusion matrix for the random forest model"
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#interpreting-models",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#interpreting-models",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Interpreting Models",
    "text": "Interpreting Models\nAethos comes equipped with automated SHAP use cases to interpret each model. You can view the force, decision, summary, and dependence plot for any model — each with customizable parameters to satisfy your use case.\nBy specifying the output_file parameter, Aethos knows to save and track this artifact for a specific model.\nlgbm.decision_plot(output_file='decision_plot.png')\n\nlgbm.force_plot(output_file='force_plot.png')\n\nlgbm.shap_get_misclassified_index()  \n# [2, 10, 21, 38, 43, 57, 62, 69, 70, 85, 89, 91, 96, 98, 108, 117, 128, 129, 139, 141, 146, 156, 165, 167, 169]  \nlgbm.force_plot(2, output_file='force_plot_2.png')\n\nlgbm.summary_plot(output_file='summary_plot.png')\n\nlgbm.dependence_plot('fare', output_file='dep_plot.png')\n\nFor a more automated experience, run:\nlgbm.interpret_model()\n\nThis displays an interactive dashboard where you can interpret your model’s prediction using LIME, SHAP, Morris Sensitivity, etc."
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#viewing-models-in-mlflow",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#viewing-models-in-mlflow",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Viewing Models in MLFlow",
    "text": "Viewing Models in MLFlow\nAs we were training models and viewing results, our experiments were automatically being tracked. If you run aethos mlflow-ui from the command line, a local MLFlow instance will be started for you to view the results and artifacts of your experiments. Navigate to localhost:10000.\n\nEach of your models and any saved artifacts are tracked and can be viewed from the UI, including parameters and metrics! To view and download model artifacts, including the pickle file for a specific model, click on the hyperlink in the date column.\n\nYou get a detailed breakdown of the metrics for the model as well as the model parameters and towards the bottom, you will see all your saved artifacts for a specific model.\n\nNote: Cross validation learning curves and mean score plots are always saved as artifacts.\nTo change the name of the experiment ( by default it is my-experiment) specify the name when initiating the Model object ( exp_name), otherwise every model will get added to my-experiment."
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#serving-models",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#serving-models",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Serving Models",
    "text": "Serving Models\nOnce you’ve decided on a model you want to serve for predictions, you can easily generate the required files to serve the model with a RESTful API using FastAPI and Gunicorn.\ndt.to_service('titanic')\n\nIf you’re familiar with MLFlow, you can always use it to serve your models as well. Open a terminal where the deployment files are being kept. I recommend moving these files to a git repository to version control both the model and served files.\nFollow the instructions to build the docker container and then run it detached.\ndocker build -t titanic:1.0.0 ./  \ndocker run -d --name titanic -p 1234:80 titanic:1.0.0\nNow navigate to localhost:1234/docs to test your API. You can now serve predictions by sending POST requests to 127.0.0.1:1234/predict.\n \nNow one big thing to note, this is running a default configuration and should not be used in production without securing it and configuring the server for production use. In the future, I will add these configurations myself so you can more or less “throw” a model straight into production with minimal configuration changes.\nThis feature is still in its infancy and one that I will actively continue to develop and improve."
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#what-else-can-you-do",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#what-else-can-you-do",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "What Else Can You Do?",
    "text": "What Else Can You Do?\nWhile this post is a semi- comprehensive guide of modelling with Aethos, you can also run statistical tests such as T-Test, Anovas, use pretrained models such as BERT and XLNet for sentiment analysis and question answering, perform extractive summarization with TextRank, train a gensim LDA model, as well as clustering, anomaly detection and regression models.\nThe full example can be seen here!"
  },
  {
    "objectID": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#feedback",
    "href": "posts/aethos_modelling/2020-03-14-aethos-modelling.html#feedback",
    "title": "Training, Tracking, Analyzing, Interpreting and Serving Models with One Line of Code",
    "section": "Feedback",
    "text": "Feedback\nI encourage all feedback about this post or Aethos. You can message me on twitter or e-mail me at sidhuashton@gmail.com.\nAny bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started."
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "",
    "text": "Prefect is a workflow management system that enables users to easily take data applications and add retries, logging, dynamic mapping, caching, failure notifications, scheduling and more — all with functional Python API. Prefect allows users to take their existing code and transform it into a DAG (Directed Acyclic Graph) with dependencies already identified [1]. It simplifies the creation of ETL pipelines and dependencies and enables users to strictly focus on the application code instead of the pipeline code (looking at you Airflow). Prefect can even create distributed pipelines to parallelize your data applications.\nDatabricks at its core is a PaaS (Platform as a Service) that delivers fully managed Spark clusters, interactive & collaborative notebooks (similar to Jupyter), a production pipeline scheduler and a platform for powering your Spark-based applications. It is integrated in both the Azure and AWS ecosystem to make working with big data simple. Databricks enables users to run their custom Spark applications on their managed Spark clusters. It even allows users to schedule their notebooks as Spark jobs. It has completely simplified big data development and the ETL process surrounding it.\nDatabricks has become such an integral big data ETL tool, one that I use every day at work, so I made a contribution to the Prefect project enabling users to integrate Databricks jobs with Prefect. In this tutorial we will go over just that — how you can incorporate running Databricks notebooks and Spark jobs in your Prefect flows."
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#prerequisites",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#prerequisites",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "Prerequisites",
    "text": "Prerequisites\nThere is no prior knowledge needed for this post however a free Prefect account is recommended to implement the example. While this post will touch on Prefect basics, it is not an in depth Prefect tutorial."
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#prefect-basics",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#prefect-basics",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "Prefect Basics",
    "text": "Prefect Basics\n\nTasks\nA task in Prefect is the equivalent of a step in your data pipeline. It is as simple as a Python function in your application or script. There are no restrictions on how simple or complex tasks can be. That being said, it’s best to follow coding best practices and develop your functions, so they only do one thing. Prefect themselves recommend this.\n\n\n\n\n\n\nNote\n\n\n\nIn general, we encourage small tasks over monolithic ones, each task should perform a discrete logical step of your workflow, but not more. 2\n\n\nBy keeping tasks small, you will get the most out of Prefect’s engine such as efficient state checkpoints.\n\n\nFlows\nA flow is what ties all your tasks and their dependencies together. It describes dependencies between tasks, their ordering and the data flow. Flows pull together tasks and make it into a pipeline rounding out your data application.\n\n\n\nPrefect Flow Visualization"
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#native-databricks-integration-in-prefect",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#native-databricks-integration-in-prefect",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "Native Databricks Integration in Prefect",
    "text": "Native Databricks Integration in Prefect\nI made a contribution to the Prefect project by the implementing the tasks DatabricksRunNow & DatabricksRunSubmit enabling seamless integration between Prefect and Databricks. Through these tasks users can externally trigger a defined Databricks job or a single run of a jar, Python script or notebook. Once a task has been executed it uses Databricks native API calls to run notebooks or Spark Jobs. When the task is running it will continue to poll the current status of the run until it’s completed. Once a task is completed it will allow for downstream tasks to run if it is successful."
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#creating-a-flow-with-databricks-tasks",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#creating-a-flow-with-databricks-tasks",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "Creating a Flow with Databricks Tasks",
    "text": "Creating a Flow with Databricks Tasks\nBefore we get started writing any code, we have to create a Prefect Secret that is going to store our Databricks connection string. From your Prefect Cloud account, click on Team from the left side menu and go to the Secrets section. This section is where you manage all the secrets for your Prefect Flows.\nTo generate the Databricks connection string you will need the host name of your Databricks instance as well as a PAT for your Databricks account. To create a Databricks PAT, follow these steps from the Databricks documentation. The connection string has to be a valid JSON object. The title of the secret has to be DATABRICKS_CONNECTION_STRING.\n\n\n\nPrefect Secret for Databricks Connection String\n\n\n\nCreating the Tasks\nLet’s start our flow by defining some common tasks that we will need to run our Databricks notebooks and Spark jobs.\nfrom prefect import task, Flow\nfrom prefect.tasks.databricks.databricks_submitjob import (\n    DatabricksRunNow,\n    DatabricksSubmitRun,\n)\nfrom prefect.tasks.secrets.base import PrefectSecret\n\nconn = PrefectSecret(\"DATABRICKS_CONNECTION_STRING\")\n# Initialize Databricks task class as a template\n# We will then use the task function to pass in unique config options & params\nRunNow = DatabricksRunNow(conn)\nSubmitRun = DatabricksSubmitRun(conn)\nWe define two task objects, RunNow and SubmitRun, to act as templates to run our Databricks jobs. We can reuse these same tasks with different configurations to easily create new Databricks jobs. Let’s create some helper tasks to dynamically create the configuration of our jobs.\n@task\ndef get_submit_config(python_params: list):\n    \"\"\"\n    SubmitRun config template for the DatabricksSubmitRun task,\n\n    Spark Python Task params must be passed as a list.\n    \"\"\"\n    return {\n        \"run_name\": \"MyDatabricksJob\",\n        \"new_cluster\": {\n          \"spark_version\": \"7.3.x-scala2.12\",\n          \"node_type_id\": \"r3.xlarge\",\n          \"aws_attributes\": {\n            \"availability\": \"ON_DEMAND\"\n          },\n          \"num_workers\": 10\n        },\n        \"spark_python_task\": {\n            \"python_file\": \"/Users/ashton/databricks_task/main.py\",\n            \"parameters\": python_params,\n        },\n    }\n\n\n@task\ndef get_run_now_config(notebook_params: dict):\n    \"\"\"\n    RunNow config template for the DatabricksSubmitRun task,\n\n    Notebook Task params must be passed as a dictionary.\n    \"\"\"\n    return {\"job_id\": 42, \"notebook_params\": notebook_params}\nThe get_submit_config task allows us to dynamically pass parameters to a Python script that is on DBFS (Databricks File System) and return a configuration to run a single use Databricks job. You can add more flexibility by creating more parameters that map to configuration options in your Databricks job configuration. The get_run_now_config executes same task except it returns a configuration for the DatabricksRunNow task to run a preconfigured Databricks Notebooks job. The schemas of both the get_run_now_config and get_submit_config match the Run Now and Runs Submit API respectively.\n\n\n\n\n\n\nNote\n\n\n\nPython file parameters must be passed as a list and Notebook parameters must be passed as a dictionary.\n\n\nNow let’s create a flow that can run our tasks.\n\n\nCreating the Flow\nWe’re going to create a flow that runs a preconfigured notebook job on Databricks, followed by two subsequent Python script jobs.\nwith Flow(\"Databricks-Tasks\", schedule=None) as flow:\n\n    run_now_config = get_run_now_config({\"param1\": \"value\"})\n    submit_config_a = get_submit_config([\"param1\"])\n    submit_config_b = get_submit_config([\"param2\"])\n\n    run_now_task = RunNow(json=run_now_config)\n\n    submit_task_a = SubmitRun(json=submit_config_a)\n\n    submit_task_b = SubmitRun(json=submit_config_b)\n\n    # Since Databricks tasks don't return any data dependencies we can leverage,\n    # we have to define the dependencies between Databricks tasks themselves\n    flow.add_edge(run_now_task, submit_task_a)\n    flow.add_edge(submit_task_a, submit_task_b)\nWe first need to create the Databricks job configuration by using our get_run_now_config and get_submit_config tasks. Pass the run now configuration to the RunNow task and the submit run configuration to the SubmitRun task through the json argument. The json parameter takes in a dictionary that matches the Run Now and Submit Run APIs mentioned above. To run more Databricks jobs we instantiate either the RunNow or SubmitRun templates we created and pass in a new json job config.\nOne of the awesome features of a Prefect flow is that it automatically builds a DAG from your tasks. It looks at task inputs as data dependencies and from that, can infer what tasks need to be completed before other tasks can run. For example, since our run_now_task has the input run_now_config, the flow builds the DAG knowing the get_run_now_config task has to run before the run_now_task.\nSome tasks don’t return data that can be used as inputs in down stream tasks. For example, the Databricks tasks only return a job ID. We can still define the inter-task dependencies of the flow by using the .add_edge function. This will add dependencies between tasks that aren’t used as inputs for further down stream tasks. For example, flow.add_edge(run_now_task, submit_task_a) says that submit_task_a is a downstream task from the run_now_task and that submit_task_a cannot run until the run_now_task has been completed. By adding the edges to the remaining Databricks task we get our final flow, which you can also view in the Prefect schematics tab.\n\n\n\nDAG of our Flow\n\n\nTo the run the flow, we call the .run() method of our flow object — flow.run(). The final flow then looks like this:\nfrom prefect import task, Flow\nfrom prefect.tasks.databricks.databricks_submitjob import (\n    DatabricksRunNow,\n    DatabricksSubmitRun,\n)\nfrom prefect.tasks.secrets.base import PrefectSecret\n\n\n@task\ndef get_submit_config(python_params: list):\n    \"\"\"\n    SubmitRun config template for the DatabricksSubmitRun task,\n\n    Spark Python Task params must be passed as a list.\n    \"\"\"\n    return {\n        \"run_name\": \"MyDatabricksJob\",\n        \"new_cluster\": {\n          \"spark_version\": \"7.3.x-scala2.12\",\n          \"node_type_id\": \"r3.xlarge\",\n          \"aws_attributes\": {\n            \"availability\": \"ON_DEMAND\"\n          },\n          \"num_workers\": 10\n        },\n        \"spark_python_task\": {\n            \"python_file\": \"/Users/ashton/databricks_task/main.py\",\n            \"parameters\": python_params,\n        },\n    }\n\n\n@task\ndef get_run_now_config(notebook_params: dict):\n    \"\"\"\n    RunNow config template for the DatabricksSubmitRun task,\n\n    Notebook Task params must be passed as a dictionary.\n    \"\"\"\n    return {\"job_id\": 42, \"notebook_params\": notebook_params}\n\n\nconn = PrefectSecret(\"DATABRICKS_CONNECTION_STRING\")\n# Initialize Databricks task class as a template\n# We will then use the task function to pass in unique config options & params\nRunNow = DatabricksRunNow(conn)\nSubmitRun = DatabricksSubmitRun(conn)\n\nwith Flow(\"Databricks-Tasks\", schedule=None) as flow:\n\n    run_now_config = get_run_now_config({\"param1\": \"value\"})\n    submit_config_a = get_submit_config([\"param1\"])\n    submit_config_b = get_submit_config([\"param2\"])\n\n    run_now_task = RunNow(json=run_now_config)\n\n    submit_task_a = SubmitRun(json=submit_config_a)\n\n    submit_task_b = SubmitRun(json=submit_config_b)\n\n    # Since Databricks tasks don't return any data dependencies we can leverage,\n    # we have to define the dependencies between Databricks tasks themselves\n    flow.add_edge(run_now_task, submit_task_a)\n    flow.add_edge(submit_task_a, submit_task_b)\n\nflow.run()\n# flow.register(\"YOUR_PROJECT\") to register your flow on the UI"
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#conclusion",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#conclusion",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "Conclusion",
    "text": "Conclusion\nYou now have all the knowledge you need to run Databricks Notebooks and Spark jobs as part of your ETL flows. For more information on Prefect and Databricks jobs, I recommend reading their documentation found here and here."
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#feedback",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#feedback",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "Feedback",
    "text": "Feedback\nAs always, I encourage any feedback about my post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help.\nYou can also reach me and follow me on Twitter at @ashtonasidhu."
  },
  {
    "objectID": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#references",
    "href": "posts/prefect_databricks/2020-11-02-prefect-databricks.html#references",
    "title": "Integrating Prefect & Databricks to Manage your Spark Jobs",
    "section": "References",
    "text": "References\n\nhttps://docs.prefect.io/core/, Prefect Documentation\nhttps://docs.prefect.io/core/getting_started/first-steps.html, Prefect Getting Started"
  },
  {
    "objectID": "posts/igviz/2020-03-27-intro-to-igviz.html",
    "href": "posts/igviz/2020-03-27-intro-to-igviz.html",
    "title": "Intro to Interactive Graph Visualizations",
    "section": "",
    "text": "Over the last couple of months I have started to explore more with Graphs and network analysis as it pertains to user behaviour in cyber security. One of the main pain points I found early on was visualizing the network, the node properties and edges in a neat, visually appealing way. Alot of the default plotting options are built on Matplotlib, which is well, let’s just say suboptimal for this type of visualization.\nInstead of playing around with the Matplotlib axes api, I wanted something a little bit more robust, easier to use out of the box, visually appealing and neat while maximizing the possible amount of information that can be displayed on the graph.\nIn comes my favourite visualization library Plotly. Plotly already had some documentation on how to visualize graphs, but it was still a fairly lengthy process. The goal was to find (or create) a uniform api that follows the .plot standard, with some customizability, but for graphs. Hence, why I made this little package called Interactive Graph Visualizations (igviz).\nTo install run pip install igviz\nCreate a random graph for demonstration purposes and assign every node a property called prop and every edge a property called “edge_prop” and make the values 12 and 3.\nNow we plot!"
  },
  {
    "objectID": "posts/igviz/2020-03-27-intro-to-igviz.html#the-basics",
    "href": "posts/igviz/2020-03-27-intro-to-igviz.html#the-basics",
    "title": "Intro to Interactive Graph Visualizations",
    "section": "The Basics",
    "text": "The Basics\n\n\n\n\n\n\nTip\n\n\n\nAll the example notebooks can be found here.\n\n\nBy default, the nodes are sized and coloured by the degree of itself. The degree of a node is just the number of edge’s (lines connecting 2 nodes) it has. You can see the degree of the node by hovering over it!\n\nfig = ig.plot(G)\nfig.show()\n\n/tmp/ipykernel_19017/633761989.py:1: DeprecationWarning: Argument `titlefont_size` is deprecated and will be removed in 0.6.0.\n  fig = ig.plot(G)\n\n\n\n                                                \n\n\n\n\n\n\n\n\nTip\n\n\n\nWhen it comes to customizability, you can change the how the nodes are sized (size_method) to either be static, based off a node’s property or your own custom sizing method.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe color’s can be changed (color_method) to be a static color (hex, plain text, etc.), based off a node’s property or your own custom color method.\n\n\nThere are more options, and more to come, but those are the ones that greatly impact the visualization and information of your graph."
  },
  {
    "objectID": "posts/igviz/2020-03-27-intro-to-igviz.html#customizing-the-graph-appearance",
    "href": "posts/igviz/2020-03-27-intro-to-igviz.html#customizing-the-graph-appearance",
    "title": "Intro to Interactive Graph Visualizations",
    "section": "Customizing the Graph Appearance",
    "text": "Customizing the Graph Appearance\nHere all the nodes are set to the same size and the colour is set to a light red while displaying the prop property on hover.\n\n\n\n\n\n\nTip\n\n\n\nTo display Node’s properties when you hover it, specify a list for the node_text parameter of node properties you want to have displayed. By default only degree is shown.\n\n\n\nfig = ig.plot(\n    G, # Your graph\n    title=\"My Graph\",\n    size_method=\"static\", # Makes node sizes the same\n    color_method=\"#ffcccb\", # Makes all the node colours black,\n    node_text=[\"prop\"], # Adds the 'prop' property to the hover text of the node\n    annotation_text=\"Visualization made by <a href='https://github.com/Ashton-Sidhu/plotly-graph'>igviz</a> & plotly.\", # Adds a text annotation to the graph\n)\n\nfig.show()\n\n/tmp/ipykernel_19017/885303569.py:1: DeprecationWarning:\n\nArgument `titlefont_size` is deprecated and will be removed in 0.6.0.\n\n\n\n\n                                                \n\n\nHere, the sizing and color method is based off the prop property of every node as well as we’re displaying the prop property on hover.\n\nfig = ig.plot(\n    G,\n    title=\"My Graph\",\n    size_method=\"prop\", # Makes node sizes the size of the \"prop\" property\n    color_method=\"prop\", # Colors the nodes based off the \"prop\" property and a color scale,\n    node_text=[\"prop\"], # Adds the 'prop' property to the hover text of the node\n)\n\nfig.show()\n\n/tmp/ipykernel_19017/554760700.py:1: DeprecationWarning:\n\nArgument `titlefont_size` is deprecated and will be removed in 0.6.0.\n\n\n\n\n                                                \n\n\nTo add your own sizing and color methods, pass in a list of a color or size pertaining to each node in the graph to the size_method and color_method parameters.\n\n\n\n\n\n\nTip\n\n\n\nTo change the colorscale, change the colorscale parameter!\n\n\n\ncolor_list = []\nsizing_list = []\n\nfor node in G.nodes():\n    size_and_color = G.degree(node) * 3\n\n    color_list.append(size_and_color)\n    sizing_list.append(size_and_color)\n\nfig = ig.plot(\n    G,\n    title=\"My Graph\",\n    size_method=sizing_list, # Makes node sizes the size of the \"prop\" property\n    color_method=color_list, # Colors the nodes based off the \"prop\" property and a color scale,\n    node_text=[\"prop\"], # Adds the 'prop' property to the hover text of the node\n)\n\nfig.show()\n\n/tmp/ipykernel_19017/1171289552.py:10: DeprecationWarning:\n\nArgument `titlefont_size` is deprecated and will be removed in 0.6.0.\n\n\n\n\n                                                \n\n\n\nLabels\nYou can also add labels to both nodes and edges as well as add hover text to the edges based on their attributes.\n\nig.plot(\n    G,\n    node_label=\"prop\", # Display the \"prop\" attribute as a label on the node\n    node_label_position=\"top center\", # Display the node label directly above the node\n    edge_text=[\"edge_prop\"], # Display the \"edge_prop\" attribute on hover over the edge\n    edge_label=\"edge_prop\", # Display the \"edge_prop\" attribute on the edge\n    edge_label_position=\"bottom center\", # Display the edge label below the edge\n)\n\n/tmp/ipykernel_19017/1570631554.py:1: DeprecationWarning:\n\nArgument `titlefont_size` is deprecated and will be removed in 0.6.0."
  },
  {
    "objectID": "posts/igviz/2020-03-27-intro-to-igviz.html#layouts",
    "href": "posts/igviz/2020-03-27-intro-to-igviz.html#layouts",
    "title": "Intro to Interactive Graph Visualizations",
    "section": "Layouts",
    "text": "Layouts\nYou can change the way your graph is oragnized and laid out by specifying a type of layout. Networkx comes with predefined layouts to use and we can apply them through layout.\nBy default, igviz looks for the pos node property and if it doesn’t exist it will default to a random layout.\nThe supported layouts are:\n\nrandom (default): Position nodes uniformly at random in the unit square. For every node, a position is generated by choosing each of dim coordinates uniformly at random on the interval [0.0, 1.0).\ncircular: Position nodes on a circle.\nkamada: Position nodes using Kamada-Kawai path-length cost-function.\nplanar: Position nodes without edge intersections, if possible (if the Graph is planar).\nspring: Position nodes using Fruchterman-Reingold force-directed algorithm.\nspectral: Position nodes using the eigenvectors of the graph Laplacian.\nspiral: Position nodes in a spiral layout.\n\n\nfig = ig.plot(\n    G,\n    title=\"My Graph\",\n    layout=\"kamada\"\n)\n\nfig.show()\n\n/tmp/ipykernel_19017/1973471548.py:1: DeprecationWarning:\n\nArgument `titlefont_size` is deprecated and will be removed in 0.6.0.\n\n\n\n\n                                                \n\n\nTo add your own pos property you can set it via the nx.set_node_attributes function.\npos_dict = {\n    0: [1, 2], # X, Y coordinates for Node 0\n    1: [1.5, 3], # X, Y coordinates for Node 1\n    ...\n}\n\nnx.set_node_attributes(G, pos_dict, \"pos\")\n\nfig = ig.plot(G)\n\nfig.show()"
  },
  {
    "objectID": "posts/igviz/2020-03-27-intro-to-igviz.html#feedback",
    "href": "posts/igviz/2020-03-27-intro-to-igviz.html#feedback",
    "title": "Intro to Interactive Graph Visualizations",
    "section": "Feedback",
    "text": "Feedback\nI encourage all feedback about this post or Igviz. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post.\nAny bug or feature requests, please create an issue on the Github repo. I welcome all feature requests and any contributions. This project is a great starter if you’re looking to contribute to an open source project — you can always message me if you need assistance getting started.\n\n\n\n\n\n\nTip\n\n\n\nIf you like the project, give it a star!"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "",
    "text": "Working on your own data science projects are a great opportunity to learn some new skills and hone existing skills, but what if you want to use technologies that you would use in industry such as Hadoop, Spark on a distributed cluster, Hive, etc. and have them all integrated? This is where the value comes from when building your own infrastructure.\nYou become familiar with the technologies, get to know the ins and outs about how it operates, debug and experience the different types of error messages and really get a sense of how the technology works over all instead of just interfacing with it. If you are also working with your own private data or confidential data in general, you may not want to upload it to an external service to do big data processing for privacy or security reasons. So, in this tutorial I’m going to walk through how to setup your own Big Data infrastructure on your own computer, home lab, etc. We’re going to setup a single node Hadoop & Hive instance and a “distributed” spark cluster integrated with Jupyter.\nEdit: Thanks to @Daniel Villanueva you can now deploy a VM with Hadoop, Spark and Hive pre-configured and ready to go through his Vagrant image. You can check it out on his Github here."
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#prerequisites",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#prerequisites",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA Debian based distro - Ubuntu, Pop-os, etc\nBasic command line knowledge helps, but not essential for installation"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-1---download-hadoop-and-hive",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-1---download-hadoop-and-hive",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Step 1 - Download Hadoop and Hive",
    "text": "Step 1 - Download Hadoop and Hive\nHadoop is easily the most common big data warehouse platform used in industry today and is a must know for any big data job. In short, Hadoop is an open-source software framework used for storing and processing Big Data in a distributed manner. You can download the latest version from here.\nHive is usually added on top of Hadoop to query the data in Hadoop in a SQL like fashion. Hive makes job easy for performing operations like\n\nData encapsulation\nAd-hoc queries\nAnalysis of huge datasets\n\nHive is slow and generally used for batch jobs only. A much faster version of Hive would be something like Impala, but for home use - it gets the job done. You can download the latest version of Hive here.\n\n\n\n\n\n\nNote\n\n\n\nMake sure you download the binary (bin) version and not the source (src) version!\n\n\n\nExtract the files to /opt\ncd ~/Downloads\ntar -C /opt -xzvf apache-hive-3.1.2-bin.tar.gz\ntar -C /opt -xzvf hadoop-3.1.3-src.tar.gz\nRename them to hive and hadoop.\ncd /opt\nmv hadoop-3.1.3-src hadoop\nmv apache-hive-3.1.2-bin hive"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-2---setup-authorized-or-password-less-ssh.",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-2---setup-authorized-or-password-less-ssh.",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Step 2 - Setup Authorized (or Password-less) SSH.",
    "text": "Step 2 - Setup Authorized (or Password-less) SSH.\nWhy do we need to do this? The Hadoop core uses Shell (SSH) to launch the server processes on the slave nodes. It requires a password-less SSH connection between the master and all conencted nodes. Otherwise, you would have to manually go to each node and start each Hadoop process.\nSince we are running a local instance of Hadoop, we can save ourselves the hassle of setting up hostnames, SSH keys and adding them to each box. If this were a distributed environment, it would also be best to create a hadoop user, but it’s not necessary for a single node setup and personal use.\nThe really easy, only suitable for home use, should not be used or done anywhere else way is:\ncat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\nNow run ssh localhost and you should be able to login without a password.\n\n\n\nSuccessful SSH login\n\n\nTo get an idea of what it takes to configure the networking and infrastructure on a distributed environment, this is a great source."
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-3---install-java-8",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-3---install-java-8",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Step 3 - Install Java 8",
    "text": "Step 3 - Install Java 8\nOne of the most important steps of this tutorial.\n{% include alert.html text=“If this is done incorrectly, it will cause a grueling number of hours debugging vague error messages just to realize the problem and solution was so simple.” %}\nHadoop has one main requirement and this is Java version 8. Funnily enough, that’s also the Java requirement for Spark, also very important.\nsudo apt-get update\nsudo apt-get install openjdk-8-jdk\nVerify the Java version.\njava -version\n\n\n\nIncorrect Java version\n\n\nIf for some reason you don’t see the output above, you need to update your default Java version.\nsudo update-alternatives --config java\n\n\n\nUpdate Java version\n\n\nChoose the number associated with Java 8.\nCheck the version again.\njava -version\n\n\n\nCorrect Java version"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-4---configure-hadoop-yarn",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-4---configure-hadoop-yarn",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Step 4 - Configure Hadoop + Yarn",
    "text": "Step 4 - Configure Hadoop + Yarn\nApache Hadoop YARN (Yet Another Resource Negotiator) is a cluster management technology. At a very basic level it helps Hadoop manage and monitor its workloads.\n\nInitial Hadoop Setup\nFirst let’s set our environment variables. These specifies where the configuration for Hadoop, Spark and Hive is located.\nnano ~/.bashrc\nAdd this to the bottom of your .bashrc file.\nexport HADOOP_HOME=/opt/hadoop\nexport HADOOP_INSTALL=$HADOOP_HOME\nexport HADOOP_MAPRED_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_HOME=$HADOOP_HOME\nexport HADOOP_HDFS_HOME=$HADOOP_HOME\nexport YARN_HOME=$HADOOP_HOME\nexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\nexport PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\n\nexport LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH\n\nexport HIVE_HOME=/opt/hive\nexport PATH=$PATH:$HIVE_HOME/bin\nSave and exit out of nano CTRL + o, CTRL + x.\nThen we need to active these changes by running source ~/.bashrc. You can also close and reopen your terminal to achieve the same result.\nNext we need to make some directories and edit permissions. Make the following directories:\nsudo mkdir -p /app/hadoop/tmp\nmkdir -p ~/hdfs/namenode\nmkdir ~/hdfs/datanode\nEdit the permissions for /app/hadoop/tmp, giving it read and write access.\nsudo chown -R $USER:$USER /app\nchmod a+rw -R /app\n\n\nConfig Files\nAll the Hadoop configuration files are located in /opt/hadoop/etc/hadoop/.\ncd /opt/hadoop/etc/hadoop\nNext we need to edit the following configuration files:\n- core-site.xml\n- hadoop-env.sh\n- hdfs-site.xml\n- mapred-site.xml\n- yarn-site.xml\ncore-site.xml\n<configuration>\n    <property>\n        <name>hadoop.tmp.dir</name>\n        <value>/app/hadoop/tmp</value>\n    <description>Parent directory for other temporary directories.</description>\n    </property>\n    <property>\n        <name>fs.defaultFS </name>\n        <value>hdfs://YOUR_IP:9000</value>\n        <description>The name of the default file system. </description>\n    </property>\n</configuration>\nhadoop.tmp.dir: Fairly self explanatory, just a directory for hadoop to store other temp directories fs.defaultFS: The IP and port of your file system to access over the network. It should be your IP so other nodes can connect to it if this were a distributed system.\nTo find your ip, type ip addr or ifconfig on the command line:\n\n\n\nFind your IP\n\n\nhadoop-env.sh\n\nIdentify the location of the Java 8 JDK, it should be similar or idential to /usr/lib/jvm/java-8-openjdk-amd64/\nAdd the following line to hadoop-env.sh: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/\n\nhdfs-site.xml\n<configuration>\n    <property>\n        <name>dfs.replication</name>\n        <value>1</value>\n        <description>Default block replication.</description>\n    </property>\n    <property>\n        <name>dfs.name.dir</name>\n        <value>file:///home/YOUR_USER/hdfs/namenode</value>\n    </property>\n    <property>\n        <name>dfs.data.dir</name>\n        <value>file:///home/YOUR_USER/hdfs/datanode</value>\n    </property>\n</configuration>\ndfs.replication: How many nodes to replicate the data on.\ndfs.name.dir: Directory for namenode blocks\ndfs.data.dir: Directory for the data node blocks\nmapred-site.xml\n<configuration>\n    <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n    <property>\n        <name>mapreduce.jobtracker.address</name>\n        <value>localhost:54311</value>\n    </property>\n    <property>\n        <name>yarn.app.mapreduce.am.env</name>\n        <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>\n    </property>\n    <property>\n        <name>mapreduce.map.env</name>\n        <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>\n    </property>\n    <property>\n        <name>mapreduce.reduce.env</name>\n        <value>HADOOP_MAPRED_HOME=$HADOOP_MAPRED_HOME</value>\n    </property>\n    <property>\n        <name>mapreduce.map.memory.mb</name>\n        <value>4096</value>\n    </property>\n    <property>\n        <name>mapreduce.reduce.memory.mb</name>\n        <value>4096</value>\n    </property>\n</configuration>\nmapreduce.framework.name: The runtime framework for executing MapReduce jobs. Can be one of local, classic or yarn.\nmapreduce.jobtracker.address: The host and port that the MapReduce job tracker runs at. If “local”, then jobs are run in-process as a single map and reduce task.\nyarn.app.mapreduce.am.env: Yarn map reduce env variable.\nmapreduce.map.env: Map reduce map env variable.\nmapreduce.reduce.env: Map reduce reduce env variable.\nmapreduce.map.memory.mb: Upper memory limit that Hadoop allows to be allocated to a mapper, in megabytes. The default is 512.\nmapreduce.reduce.memory.mb: Upper memory limit that Hadoop allows to be allocated to a reducer, in megabytes. The default is 512.\nyarn-site.xml\n<configuration>\n<!-- Site specific YARN configuration properties -->\n    <property>\n        <name>yarn.resourcemanager.hostname</name>\n        <value>localhost</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n    <property>\n        <name>yarn.nodemanager.resource.memory-mb</name>\n        <value>16256</value>\n    </property>\n    <property>\n        <name>yarn.app.mapreduce.am.resource.mb</name>\n        <value>4096</value>\n    </property>\n    <property>\n        <name>yarn.scheduler.minimum-allocation-mb</name>\n        <value>4096</value>\n    </property>\n</configuration>\nyarn.resourcemanager.hostname: The hostname of the RM. Could also be an ip address of a remote yarn instance.\nyarn.nodemanager.aux-services: Selects a shuffle service that needs to be set for MapReduce to run.\nyarn.nodemanager.resource.memory-mb: Amount of physical memory, in MB, that can be allocated for containers. For reference, I have 64GB of RAM on my machine. If this value is too low, you won’t be able to process large files, getting a FileSegmentManagedBuffer error.\nyarn.app.mapreduce.am.resource.mb: This property specify criteria to select resource for particular job. Any nodemanager which has equal or more memory available will get selected for executing job.\nyarn.scheduler.minimum-allocation-mb: The minimum allocation for every container request at the RM, in MBs. Memory requests lower than this won’t take effect, and the specified value will get allocated at minimum.\n\n\nStart Hadoop\nBefore we start Hadoop we have to format the namenode:\nhdfs namenode -format\nNow we’re good to start Hadoop! Run the following commands:\nstart-dfs.sh\nstart-yarn.sh\nTo ensure everything has started run the following commands:\nss -ln | grep 9000\n\n\n\nSS output\n\n\njps\n\n\n\njps output\n\n\nYou can now also access the Hadoop web UI at localhost:9870.\n\n\n\nHadoop web UI\n\n\nYou can also access the Yarn web UI at localhost:8088.\n\n\n\nYarn web UI"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-5---setup-hive",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-5---setup-hive",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Step 5 - Setup Hive",
    "text": "Step 5 - Setup Hive\nNow that we have Hadoop up and running, let’s install Hive on top of it.\nFirst let’s make a directory in Hadoop where our Hive tables are going to be stored.\nhdfs dfs -mkdir -p /user/hive/warehouse\nConfigure permissions.\nhdfs dfs -chmod -R a+rw /user/hive\n\nSetup a Metastore\nThe Hive Metastore is the central repository of Hive Metadata. It stores the meta data for Hive tables and relations (Schema and Locations etc). It provides client access to this information by using metastore service API. There are 3 different types of metastores:\n\nEmbedded Metastore: Only one Hive session can be open at a time.\nLocal Metastore: Multiple Hive sessions, have to connect to an external DB.\nRemote Metastore: Multiple Hive sessions, interact with the metastore using Thrift API, better security and scalability.\n\nTo read up on the difference between each type of metastore in more detail, this is a great link.\nIn this guide we’re going to be setting up a remote metastore using a MySQL DB.\nsudo apt update\nsudo apt install mysql-server\nsudo mysql_secure_installation\nRun the following commands:\nsudo mysql\nCREATE DATABASE metastore;\nCREATE USER 'hive'@'%' IDENTIFIED BY 'PW_FOR_HIVE';\nGRANT ALL ON metastore.* TO 'hive'@'%' WITH GRANT OPTION;\nReplace PW_FOR_HIVE with the password you want to use for the hive user in MySQL.\nDownload the MySQL Java Connector:\nwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.19.tar.gz\ntar -xzvf mysql-connector-java-8.0.19.tar.gz\ncd mysql-connect-java-8.0.19\ncp mysql-connector-java-8.0.19.jar /opt/hive/lib/\n\n\nEdit hive-site.xml\nNow edit /opt/hive/conf/hive-site.xml:\n<configuration>\n        <property>\n                <name>javax.jdo.option.ConnectionURL</name>\n                <value>jdbc:mysql://YOUR_IP:3306/metastore?createDatabaseIfNotExist=true&amp;useLegacyDatetimeCode=false&amp;serverTimezone=UTC</value>\n                <description>metadata is stored in a MySQL server</description>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionDriverName</name>\n                <value>com.mysql.jdbc.Driver</value>\n                <description>MySQL JDBC driver class</description>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionUserName</name>\n                <value>hive</value>\n                <description>user name for connecting to mysql server</description>\n        </property>\n        <property>\n                <name>javax.jdo.option.ConnectionPassword</name>\n                <value>PW_FOR_HIVE</value>\n                <description>password for connecting to mysql server</description>\n        </property>\n</configuration>\nReplace YOUR_IP with the local ip address. Replace PW_FOR_HIVE with the password you initiated for the hive user earlier.\n\n\nInitialize Schema\nNow let’s make MySQL accessible from anywhere on your network.\nsudo nano /etc/mysql/mysql.conf.d/mysqld.cnf\nChange bind-address to 0.0.0.0.\nRestart the service for the changes take effect: sudo systemctl restart mysql.service\nFinally, run schematool -dbType mysql -initSchema to initialize the schema in the metastore database.\n\n\nStart Hive Metastore\nhive --service metastore\n\n\nTesting Hive\nFirst start up Hive from the command line by calling hive.\nLet’s create a test table:\nCREATE TABLE IF NOT EXISTS test_table\n (col1 int COMMENT 'Integer Column',\n col2 string COMMENT 'String Column')\n COMMENT 'This is test table'\n ROW FORMAT DELIMITED\n FIELDS TERMINATED BY ','\n STORED AS TEXTFILE;\nThen insert some test data.\nINSERT INTO test_table VALUES(1,'aaa');\nThen we can view the data from the table.\nSELECT * FROM test_table;"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-6---setup-spark",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#step-6---setup-spark",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Step 6 - Setup Spark",
    "text": "Step 6 - Setup Spark\nSpark is a general-purpose distributed data processing engine that is suitable for use in a wide range of circumstances. On top of the Spark core data processing engine, there are libraries for SQL, machine learning, graph computation, and stream processing, which can be used together in an application. In this tutorial we’re going to setup a standalone Spark cluster using Docker and have it be able to spin up any number of workers. This reasoning behind this is we want to simulate a remote cluster and some of the configuration required for it.\n\n\n\n\n\n\nNote\n\n\n\nIn a production setting, Spark is usually going to be configured to use Yarn and the resources already allocated for Hadoop.\n\n\nFirst we need to create the Docker file. We’re going to use Spark version 2.4.4 in this tutorial but you can change it to 2.4.5 if you want the latest version and it also ships with Hadoop 2.7 to manage persistence and book keeping between nodes. In a production setting, Spark is often configured with Yarn to use the existing Hadoop environment and resources, since we only have Hadoop on one node, we’re going to run a spark standalone cluster. To configure Spark to run with Yarn requires minimal changes and you can see the difference in setup here.\n\nSetup Standalone Cluster\nnano Dockerfile\nFROM python:3.7-alpine\n\nARG SPARK_VERSION=2.4.4\nARG HADOOP_VERSION=2.7\n\nRUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n && tar xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C / \\\n && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \\\n && ln -s /spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /spark\n\nRUN apk add shell coreutils procps\nRUN apk fetch openjdk8\nRUN apk add openjdk8\nRUN pip3 install ipython\n\nENV PYSPARK_DRIVER_PYTHON ipython\nNow we want to spin up a Spark master and N number of spark workers. For this we’re going to use docker-compose.\nnano docker-compose.yml\nversion: \"3.3\"\nnetworks:\n  spark-network:\nservices:\n  spark-master:\n    build: .\n    container_name: spark-master\n    hostname: spark-master\n    command: >\n      /bin/sh -c '\n      /spark/sbin/start-master.sh\n      && tail -f /spark/logs/*'\n    ports:\n      - 8080:8080\n      - 7077:7077\n    networks:\n      - spark-network\n  spark-worker:\n    build: .\n    depends_on:\n      - spark-master\n    command: >\n      /bin/sh -c '\n      /spark/sbin/start-slave.sh $$SPARK_MASTER\n      && tail -f /spark/logs/*'\n    env_file:\n      - spark-worker.env\n    environment:\n      - SPARK_MASTER=spark://spark-master:7077\n      - SPARK_WORKER_WEBUI_PORT=8080\n    ports:\n      - 8080\n    networks:\n      - spark-network\nFor the master container we’re exposing port 7077 for our applications to connect to and port 8080 for the Spark job UI. For the worker we’re connecting to our Spark master through the environment variables.\nFor more options to configure the spark worker, we add them to spark-worker.env file.\nnano spark-worker.env\nSPARK_WORKER_CORES=3\nSPARK_WORKER_MEMORY=8G\nIn this configuration, each worker will use 3 cores and have 8GB of memory. Since my machine has 6 cores, we’re going to start up 2 workers. Change these values so that it is relative to your machine. For example, if your machine only has 16GB of RAM, a good memory value might be 2 or 4GB. For a full list of environment variables and more information on stand alone mode, you can read the full documentation here. If you’re wondering about executor memory, that set when submitting or starting applications.\ndocker-compose build\ndocker-compose up -d --scale spark-worker=2\nNow spark is up and running and you can view the web UI at localhost:8080!\n\n\n\nSpark web UI\n\n\n\n\nInstall Spark Locally\nOn your local machine, or any machine that’s going to be creating or using Spark, Spark needs to be installed. Since we are setting up a remote Spark cluster we have install it from the source. We’re going to use PySpark for this tutorial because I most of the time I use Python for my personal projects.\nYou can download Spark from here.\n\n\n\n\n\n\nImportant\n\n\n\nEnsure you download the same version you installed on your master. For this tutorial it’s version 2.4.4\n\n\nwget https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz\ntar -C /opt -xzvf spark-2.4.4-bin-hadoop2.7.tgz\nSetup the Spark environment variables, nano ~/.bashrc\nexport SPARK_HOME=/opt/spark\nexport PATH=$SPARK_HOME/bin:$PATH\n\nexport PYSPARK_DRIVER_PYTHON=\"jupyter\"\nexport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\nexport PYSPARK_PYTHON=python3\n\n\n\n\n\n\nNote\n\n\n\nIf you prefer Jupyter Lab, change ‘notebook’, to ‘lab’ for PYSPARK_DRIVER_PYTHON_OPTS.\n\n\n\n\nConfig Files\nTo configure Spark to use our Hadoop and Hive we need to have the config files for both in the Spark config folder.\ncp $HADOOP_HOME/etc/hadoop/core-site.xml /opt/spark/conf/\ncp $HADOOP_HOME/etc/hadoop/hdfs-site.xml /opt/spark/conf/\nnano /opt/spark/conf/hive-site.xml\n<configuration>\n        <property>\n                <name>hive.metastore.uris</name>\n                <value>thrift://YOUR_IP:9083</value>\n        </property>\n        <property>\n                <name>spark.sql.warehouse.dir</name>\n                <value>hdfs://YOUR_IP:9000/user/hive/warehouse</value>\n        </property>\n</configuration>\nhive.metastore.uris: Tells Spark to interact with the Hive metastore using the Thrift API. spark.sql.warehouse.dir: Tells Spark where our Hive tables are located in HDFS.\n\n\nInstall PySpark\npip3 install pyspark==2.4.4 or replace 2.4.4 with whatever version you installed on your spark master.\nTo run PySpark connecting to our distributed cluster run:\npyspark --master spark://localhost:7077, you can also replace localhost with your ip or a remote ip.\nThis will start up a Jupyter Notebook with the Spark Context pre defined. As a result, we now have a single environment to analyze data with or without Spark.\nBy default the executor memory is only ~1GB (1024mb). To increase it start pyspark with the following command:\npyspark --master spark://localhost:7077 --executor-memory 7g\nThere is a 10% overhead per executor in Spark so the most we could assign is 7200mb, but to be safe and have a nice round number we’ll go with 7."
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#test-integrations",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#test-integrations",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Test Integrations",
    "text": "Test Integrations\nBy default a SparkContext is automatically created and the variable is sc.\nTo read from our previously created hive table.\nfrom pyspark.sql import HiveContext\n\nhc = HiveContext(sc)\n\nhc.sql(\"show tables\").show()\n\nhc.sql(\"select * from test_table\").show()\nTo read a file from Hadoop the command would be:\nsparksession = SparkSession.builder.appName(\"example-pyspark-read-and-write\").getOrCreate()\ndf = (sparksession\n    .read\n    .format(\"csv\")\n    .option(\"header\", \"true\")\n    .load(\"hdfs://YOUR_IP:9000/PATH_TO_FILE\")\n)"
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#practical-hadoop-use-cases",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#practical-hadoop-use-cases",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Practical Hadoop Use Cases",
    "text": "Practical Hadoop Use Cases\nBesides storing data, Hadoop is also utilized as a Feature Store. Let’s say you’re apart of a team or organization and they have multiple models. For each model there is a data pipeline that ingests raw data, computes and transforms the data into features. For one or two models this is perfectly fine, but what if you have multiple models? What if across those models features are being reused (i.e log normalized stock prices)?\nInstead of each data pipeline recomputing the same features, we can create a data pipeline that computes the features once and store it in a Feature Store. The model can now pull features from the Feature Store without any redundant computation. This reduces the number of redundant computations and transformations throughout your data pipelines!\n\n\n\nBasic Feature Store concept\n\n\nFeature Stores also help with the following issues:\n\nFeatures are not reused. A common obstacle data scientists face is spending time redeveloping features instead of using previously developed features or ones developed by other teams. Feature stores allow data scientists to avoid repeat work.\nFeature definitions vary. Different teams at any one company might define and name features differently. Moreover, accessing the documentation of a specific feature (if it exists at all) is often challenging. Feature stores address this issue by keeping features and their definitions organized and consistent. The documentation of the feature store helps you create a standardized language around all of the features across the company. You know exactly how every feature is computed and what information it represents.\nThere is inconsistency between training and production features. Production and research environments often use different technologies and programming languages. The data streaming in to the production system needs to be processed into features in real time and fed into a machine learning model.\n\nIf you want to take a look at a Feature Store and get started for free, I recommend StreamSQL. StreamSQL allows you to stream your data from various sources such as HDFS, local file system, Kafka, etc. and create a data pipeline that can feed your model! It has the ability to save the feature store online or on your local HDFS for you to train your models. It also does the service of creating your test (hold out) set for you as well. They have a well documented API and is consistently improving upon it."
  },
  {
    "objectID": "posts/bigdata_tut/2020-04-17-big-data-setup.html#feedback",
    "href": "posts/bigdata_tut/2020-04-17-big-data-setup.html#feedback",
    "title": "Tutorial: How to Setup your own Big Data Environment with Hadoop, Spark and Hive for Data Science",
    "section": "Feedback",
    "text": "Feedback\nI encourage all feedback about this post. You can e-mail me at sidhuashton@gmail.com or leave a comment on the post if you have any questions or need any help.\nYou can also reach me and follow me on Twitter at @ashtonasidhu."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "",
    "text": "Open source projects are a pillar for the data industry, without them projects such as Pandas, Numpy Sci-kit Learn and the entire big data Apache stack would not exist. Hacktoberfest is a month long event where developers can contribute to open source projects on Github and receive free swag. To be eligible to receive the free swag, you must commit four successful Pull Requests on projects that have opted in to Hacktoberfest. To try and easily get the free swag, some contributors submit non meaningful contributions, leading to unhappy project maintainers.\nIn this post we’ll go through how you can give back to the community and make a meaningful contributions open source projects."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#how-i-got-into-open-source",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#how-i-got-into-open-source",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "How I got into Open Source",
    "text": "How I got into Open Source\nI first got into Open Source when I was working with two projects, interpret and pandas-summary. In both cases the usage of the projects had not completely covered my use cases, albeit they were very minor.\nWith interpret I was trying to use the library to explain/interpret a Sklearn classifier. The underlying issue is that the .predict method returns an integer (0, 1, etc.) for classifiers. When these integer values were used to calculate the Morris Sensitivity, an error was thrown saying that the data had to be a float. In this case, the fix was simple - cast the integers returned as integers by the Sklearn classifier to a float! Before the fix, anyone who tried to interpret a Sklearn classifier would get an error and this simple contribution allowed all users to interpret Sklearn classification models.\nWith pandas-summary, I was using this library to automate descriptive and statistical analysis of column data. The only issue was that every time I used the library, a histogram was always plotted, which was a nuisance. As a quality of life fix, I added a simple flag that users could specify if they want to plot the histogram.\nMy first two contributions were nothing special, and were quite simple, but from these two contributions I learned the basics of how to make a Pull Request to a remote project, follow contribution guidelines as well as interact & communicate with project maintainers via Github, Slack, etc.\nFrom there I went on to be a major contributor of the pandas-bokeh library, contributed bug fixes and UI improvements to Nteract and added major feature integrations to Prefect."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#tips",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#tips",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "Tips",
    "text": "Tips\n\nLearn how to make a Pull Request\nA pull request is a method of contributing your code into a project. To start off, find a project you want to contribute to and fork the project by clicking the fork button at the top right corner of the project page. This will create a copy of the project in its current state under your repositories.\n\n\n\nForking a Project\n\n\nOnce you have forked the project, navigate to the repository under your Projects in Github. Under the project name it should say forked from ....\nClone the forked project to your local computer.\nCreate a new branch for you change or bug fix: git checkout -b your_branch_name.\nMake your changes, commit them following the Contribution Guidelines of the project.\nPush your changes to your forked project repository: git push -u origin your_branch_name.\nNavigate to the forked repository project page. You will see a prompt to create a pull request. If you don’t navigate to the pull requests tab and create one from there by selecting New Pull Request.\n\n\n\nCreating a Pull Request\n\n\nFill out the pull request template, if there is one, or one that is outlined in the Contributing guidelines. Once that is completed, your pull request is good to go and wait for feedback!\n\n\nStart with a library you use often, big or small\nThe first step towards your first open source contribution is to pick a project you want to contribute to. The best way that I have experienced is to contribute to projects you use often or are actively using. If you have extensively used a project, you have probably come across some across some bugs or enhancements that will improve the quality of the project. This is how I started my journey in Open Source, by trying to improve projects that I was using daily.\n\n\nLook at existing Github Issues\nIf you are looking for an issue or way to contribute to a project, a good place to start is Github’s built in Issues tab. This is where users and project maintainers can log bugs and feature enhancements. Project maintainers will go through these issues and tag them, gather more information, add meta data, etc. One of the tags they will add is a “good first issue” tag to inform potential contributors that this issue is good for first time contributors or contributors who are new to the project.\n\n\n\nFirst Time Issues\n\n\nThese issues are recommended for contributors who are either new to open source or to the project itself to help them get started and make a contribution. Leverage these if you can’t find your own bug to fix or enhancement to add.\n\n\nRead the Contribution Guidelines\nThere is nothing worse than putting in all this work of finding a project, isolating a bug or developing a new feature and to have it rejected or not even looked at because you didn’t follow coding, formatting or commit message standards. The good news is that all of these are available, usually in a Contribution section in the projects README or in the Contributing Guidelines section of the project. Projects will often have automated formatting checks that run when you create a pull request and your pull request won’t often be looked at until these basic checks are passed.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t see a contribution section or a project doesn’t have Contributing Guidelines, don’t just do whatever you want. 1) Follow the coding & documentation style in the project as close as you can. 2) Follow coding & documentation best practices.\n\n\n\n\nEvery Contribution Matters\nEvery contribution in open source matters regardless how big or small. Whether it’s from a usability perspective or reading the documentation, if you are experiencing a bug or a grievance with a project there are others who are experiencing it as well. Documentation is a large component of open source software as every project needs it. It is often a great place to start contributing to a project as you gain an understanding of what a project is about. It provides background information of design decisions and considerations of the project, which will in turn help you in understand the code.\nDocumentation is the first place users go to find information and the more thoroughly documented a project is, the more of a user base it will have. Developers love documented projects and when a project doesn’t have documentation, or has poor documentation, a developer will think twice before adding it to their work flow. Adding documentation or fixing even the smallest bug may impact hundreds or even thousands of users who use that project every day and many will thank you for it."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#what-happens-next",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#what-happens-next",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "What Happens Next",
    "text": "What Happens Next\nVery rarely will your contribution get merged right away. Within a couple of days someone from the project team will comment their feedback or notify you that they are reviewing your pull request. Address the comments, ask questions, clarify anything you do not understand, make the proposed changes, if there are any, and your change with get merged shortly after!\nIf you do not receive any feedback on your pull request within a week, message a project maintainer and politely ask them what the status is. On larger projects there are often tons of pull requests and they may have forgotten about the pull request or have not got around to reviewing it yet.\nIf at this point you have not received a reply, which does not happen often (has never happened to me), take the skills and learning points from this project and move on to the next project. Once you make the pull request and have messaged the project maintainers, the rest is out of your hands. This is the only real unfortunate part of open source and one you should not take to heart."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#benefits",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#benefits",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "Benefits",
    "text": "Benefits\n\nBecome a better Engineer or Scientist\nWhether you are a Data Engineer, ML Engineer or Data Scientist contributing to Open Source will help you become better and progress in your field. From understanding how projects are built and structured, gaining deep intricate knowledge of a key library, navigating large code bases, writing production level code or even just learning a new method to solve a problem. All of these skills will translate directly into your profession or your next project.\n\n\nMeet new Engineers, Developers & Scientists\nThe greatest benefit of contributing to Open Source is the opportunity to work and interact with the minds who created a tool that is used by thousands of people world wide. You get direct insight into how they they created a solution to solve a widespread problem. Furthermore, you may end up connecting, bounce ideas off one another and collaborate on future projects together. Personally, I’ve connected with project maintainers whose project I have contributed to and have kept in touch with them on Twitter and LinkedIn."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#conclusion",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#conclusion",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "Conclusion",
    "text": "Conclusion\nToday you may not be able to contribute a new feature, but being around the project, reading the code, reading the documentation, all of it gives you insight into the project. From there, a small contribution to documentation, might lead to a bug fix that was documented, which leads to a higher understanding of the project, which then leads to your first feature.\nThe moral of the story is, contribute in any way you can and eventually you will reach that goal of developing the new feature, starting your own OS project and even becoming a key contributor to a project."
  },
  {
    "objectID": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#feedback",
    "href": "posts/hacktoberfest/2020-10-08-hacktoberfest-help.html#feedback",
    "title": "A Data Scientist’s Guide to Hacktoberfest",
    "section": "Feedback",
    "text": "Feedback\nI encourage any and all feedback about any of my posts. You can message me on twitter or e-mail me at sidhuashton@gmail.com."
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "",
    "text": "Jupyter Notebook provides a great platform to produce human-readable documents containing code, equations, analysis, and their descriptions. Some even consider it a powerful development when combining it with NBDev. For such an integral tool, the out of the box start up is not the best. Each use requires starting the Jupyter web application from the command line and entering your token or password. The entire web application relies on that terminal window being open. Some might “daemonize” the process and then use nohup to detach it from their terminal, but that’s not the most elegant and maintainable solution.\nLucky for us, Jupyter has already come up with a solution to this problem by coming out with an extension of Jupyter Notebooks that runs as a sustainable web application and has built-in user authentication. To add a cherry on top, it can be managed and sustained through Docker allowing for isolated development environments.\nBy the end of this post we will leverage the power of JupyterHub to access a Jupyter Notebook instance which can be accessed without a terminal, from multiple devices within your network, and a more user friendly authentication method."
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html#prerequisites",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html#prerequisites",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "Prerequisites",
    "text": "Prerequisites\nA basic knowledge of Docker and the command line would be beneficial in setting this up.\nI recommend doing this on the most powerful device you have and one that is turned on for most of the day, preferably all day. One of the benefits of this setup is that you will be able to use Jupyter Notebook from any device on your network, but have all the computation happen on the device we configure."
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html#what-is-jupyter-hub",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html#what-is-jupyter-hub",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "What is Jupyter Hub",
    "text": "What is Jupyter Hub\nJupyterHub brings the power of notebooks to groups of users. The idea behind JupyterHub was to scale out the use of Jupyter Notebooks to enterprises, classrooms, and large groups of users. Jupyter Notebook, however, is supposed to run as a local instance, on a single node, by a single developer. Unfortunately, there was no middle ground to have the usability and scalability of JupyterHub and the simplicity of running a local Jupyter Notebook. That is, until now.\nJupyterHub has pre-built Docker images that we can utilize to spawn a single notebook on a whim, with little to no overhead in technical complexity. We are going to use the combination of Docker and JupyterHub to access Jupyter Notebooks from anytime, anywhere, at the same URL."
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html#architecture",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html#architecture",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "Architecture",
    "text": "Architecture\nThe architecture of our JupyterHub server will consist of 2 services: JupyterHub and JupyterLab. JupyterHub will be the entry point and will spawn JupyterLab instances for any user. Each of these services will exist as a Docker container on the host."
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html#building-the-docker-images",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html#building-the-docker-images",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "Building the Docker Images",
    "text": "Building the Docker Images\nTo build our at-home JupyterHub server we will use the pre-built Docker images of JupyterHub & JupyterLab.\n\nDockerfiles\nThe JupyterHub Docker image is simple.\nFROM jupyterhub/jupyterhub:1.2\n\n# Copy the JupyterHub configuration in the container\nCOPY jupyterhub_config.py .\n\n# Download script to automatically stop idle single-user servers\nCOPY cull_idle_servers.py .\n\n# Install dependencies (for advanced authentication and spawning)\nRUN pip install dockerspawner\nWe use the pre-built JupyterHub Docker Image and add our own configuration file to stop idle servers, cull_idle_servers.py. Lastly, we install additional packages to spawn JupyterLab instances via Docker.\n\n\nDocker Compose\nTo bring everything together, let’s create a docker-compose.yml file to define our deployments and configuration.\nversion: '3'\n\nservices:\n  # Configuration for Hub+Proxy\n  jupyterhub:\n    build: .                # Build the container from this folder.\n    container_name: jupyterhub_hub   # The service will use this container name.\n    volumes:                         # Give access to Docker socket.\n      - /var/run/docker.sock:/var/run/docker.sock\n      - jupyterhub_data:/srv/jupyterlab\n    environment:                     # Env variables passed to the Hub process.\n      DOCKER_JUPYTER_IMAGE: jupyter/tensorflow-notebook\n      DOCKER_NETWORK_NAME: ${COMPOSE_PROJECT_NAME}_default\n      HUB_IP: jupyterhub_hub\n    ports:\n      - 8000:8000\n    restart: unless-stopped\n\n  # Configuration for the single-user servers\n  jupyterlab:\n    image: jupyter/tensorflow-notebook\n    command: echo\n\nvolumes:\n  jupyterhub_data:\nThe key environment variables to note are DOCKER_JUPYTER_IMAGE and DOCKER_NETWORK_NAME. JupyterHub will create Jupyter Notebooks with the images defined in the environment variable.For more information on selecting Jupyter images you can visit the following Jupyter documentation.\nDOCKER_NETWORK_NAME is the name of the Docker network used by the services. This network gets an automatic name from Docker Compose, but the Hub needs to know this name to connect the Jupyter Notebook servers to it. To control the network name we use a little hack: we pass an environment variable COMPOSE_PROJECT_NAME to Docker Compose, and the network name is obtained by appending _default to it.\nCreate a file called .env in the same directory as the docker-compose.yml file and add the following contents:\nCOMPOSE_PROJECT_NAME=jupyter_hub\n\n\nStopping Idle Servers\nSince this is our home setup, we want to be able to stop idle instances to preserve memory on our machine. JupyterHub has services that can run along side it and one of them being jupyterhub-idle-culler. This service stops any instances that are idle for a prolonged duration.\nTo add this servive, create a new file called cull_idle_servers.py and copy the contents of jupyterhub-idle-culler project into it.\n\n\n\n\n\n\nNote\n\n\n\nEnsure cull_idle_servers.py is in the same folder as the Dockerfile.\n\n\nTo find out more about JupyterHub services, check out their official documentation on them.\n\n\nJupyterhub Config\nTo finish off, we need to define configuration options such, volume mounts, Docker images, services, authentication, etc. for our JupyterHub instance.\nBelow is a simple jupyterhub_config.py configuration file I use.\nimport os\nimport sys\n\nc.JupyterHub.spawner_class = 'dockerspawner.DockerSpawner'\nc.DockerSpawner.image = os.environ['DOCKER_JUPYTER_IMAGE']\nc.DockerSpawner.network_name = os.environ['DOCKER_NETWORK_NAME']\nc.JupyterHub.hub_connect_ip = os.environ['HUB_IP']\nc.JupyterHub.hub_ip = \"0.0.0.0\" # Makes it accessible from anywhere on your network\n\nc.JupyterHub.admin_access = True\n\nc.JupyterHub.services = [\n    {\n        'name': 'cull_idle',\n        'admin': True,\n        'command': [sys.executable, 'cull_idle_servers.py', '--timeout=42000']\n    },\n]\n\nc.Spawner.default_url = '/lab'\n\nnotebook_dir = os.environ.get('DOCKER_NOTEBOOK_DIR') or '/home/jovyan/work'\nc.DockerSpawner.notebook_dir = notebook_dir\nc.DockerSpawner.volumes = {\n    '/home/sidhu': '/home/jovyan/work'\n}\nTake note of the following configuration options:\n\n'command': [sys.executable, 'cull_idle_servers.py', '--timeout=42000'] : Timeout is the number of seconds until an idle Jupyter instance is shut down.\nc.Spawner.default_url = '/lab': Uses Jupyterlab instead of Jupyter Notebook. Comment out this line to use Jupyter Notebook.\n'/home/sidhu': '/home/jovyan/work': I mounted my home directory to the JupyterLab home directory to have access to any projects and notebooks I have on my Desktop. This also allows us to achieve persistence in the case we create new notebooks, they are saved to our local machine and will not get deleted if our Jupyter Notebook Docker container is deleted.\n\nRemove this line if you do not wish to mount your home directory and do not forget to change sidhu to your user name."
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html#start-the-server",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html#start-the-server",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "Start the Server",
    "text": "Start the Server\nTo start the server, simply run docker-compose up -d, navigate to localhost:8000 in your browser and you should be able to see the JupyterHub landing page.\n\nTo access it on other devices on your network such asva laptop, an iPad, etc, identify the IP of the host machine by running ifconfig on Unix machines & ipconfig on Windows.\n\nFrom your other device, navigate to the IP you found on port 8000: http://IP:8000 and you should see the JupyterHub landing page!\n\nAuthenticating\nThat leaves us with the last task of authenticating to the server. Since we did not set up a LDAP server or OAuth, JupyterHub will use PAM (Pluggable Authentication Module) authentication to authenticate users. This means JupyterHub uses the user name and passwords of the host machine to authenticate.\nTo make use of this, we will have to create a user on the JupyterHub Docker container. There are other ways of doing this such as having a script placed on the container and executed at container start up but we will do it manually as an exercise. If you tear down or rebuild the container you will have to recreate users.\n\n\n\n\n\n\nImportant\n\n\n\nI do not recommend hard coding user credentials into any script or Dockerfile.\n\n\n\nFind the JupyterLab container ID: docker ps -a\n\n\n\n“SSH” into the container: docker exec -it $YOUR_CONTAINER_ID bash\nCreate a user and follow the terminal prompts to create a password: useradd $YOUR_USERNAME\nSign in with the credentials and you’re all set!\n\n\nYou now have a ready to go Jupyter Notebook server that can be accessed from any device, in the palm of your hands! Happy Coding!"
  },
  {
    "objectID": "posts/jupyterhub/2020-10-03-jupyterhub.html#feedback",
    "href": "posts/jupyterhub/2020-10-03-jupyterhub.html#feedback",
    "title": "Tutorial: Stop Running Jupyter Notebooks from your Command Line!",
    "section": "Feedback",
    "text": "Feedback\nI encourage any and all feedback about any of my posts and tutorials. You can message me on twitter or e-mail me at sidhuashton@gmail.com."
  }
]